# Data Engineering Projects

Welcome to my Data Engineering Projects repository! This space hosts some projects related to data engineering, each developed to address specific challenges and enhance data processing capabilities.

## Projects
- [**Project 1: Redshift Catalog**](https://github.com/alice-thomaz/Data-Engineering/tree/master/P1_Redshift_Catalog)
  - This project aims to provide visibility into the system catalog tables generated by Redshift. The use of these tables has played a crucial role in cataloging, monitoring, and governing our data warehouse. As detailed in the article linked to the project, the process of ingesting these tables may pose some challenges, and I hope that the provided code proves helpful.
  - Data Stack: DBT, Redshift.
  - Medium Article: [Catalog Tables in Redshift: Enhancing Governance of Your Data Warehouse.](https://medium.com/@alice_thomaz/ee03daf5bcad)

- [**Project 2: Databricks x BigQuery**](https://github.com/alice-thomaz/Data-Engineering/tree/master/P2_Databricks_x_BigQuery)
  - This project aims to facilitate data transfer between Databricks and BigQuery. This process plays a crucial role in the migration project to Databricks, considering that we still have active processes and pipelines in Google Cloud.
  - Data Stack: Databricks, BigQuery.
  - Medium Article: [Data Transfer Between Databricks and BigQuery with PySpark.](https://medium.com/@alice_thomaz/109eb89a2457)

- [**Project 3: Lambda - Redshift x MySQL**](https://github.com/alice-thomaz/Data-Engineering/tree/master/P3_Redshift_x_MySQL)
  - This project aims to facilitate data transfer between Redshift and MySQL. This process plays a crucial role in migrating analytical data to our production environment, allowing it to be reflected on our official product platform. This enhances client access to the content.
  - Data Stack: Redshift, Mysql, Lambda, AWS.
  - Medium Article: [Data Transfer Between Redshift and MySQL with Python.](https://medium.com/@alice_thomaz/0bfa0003cee9)

- [**Project 4: Lambdas - Multi-Cloud Campaign**](https://github.com/alice-thomaz/Data-Engineering/tree/master/P4_MultiCloud_Campaign)
  - The project aims to provide business areas with data as close to real-time as possible for the Black Friday campaign, utilizing a final visualization in Power BI, without increasing the company's cloud environment costs. To achieve this, we have adapted our existing architecture to meet all requirements during the campaign period. This repository contains the code used in the main Lambda functions of the multi-cloud project:
    - Transferring files from SharePoint to Redshift.
    - Updating Analysis Services.
    - Transferring data from AWS S3 to Azure Blob.
    - Transferring data from Redshift to S3.
  - Data Stack: Redshift, S3, Lambda, AWS, Analysis Services, Azure Blob, Azure, SharePoint.
  - Medium Article: [Maximizing Data Architecture Efficiency with AWS and Azure.](https://medium.com/@alice_thomaz/146395ca42b3)

## Getting Started
1. Choose a project from the list above.
2. Follow the provided link to access the project repository.
3. Review the project-specific README for instructions.

## Learn More
- [LinkedIn](https://www.linkedin.com/in/alice-thomaz-b6b52b14a/): Connect with me on LinkedIn to stay updated on the latest developments, and feel free to contribute and provide feedback.
- [Medium](https://medium.com/@alice_thomaz): Explore detailed articles about each project on Medium. Gain insights into motivation, design decisions, and implementation details.
